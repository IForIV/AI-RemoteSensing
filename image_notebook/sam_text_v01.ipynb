{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXSOj01/I2YBoOSO5VjxGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasOsco/AI-RemoteSensing/blob/main/image_notebook/sam_text_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code includes in part the use of the `Language Segment-Anything (LangSAM)` package. Developed by Engineer Luca Medeiros, LangSAM is an open-source project designed to combine the capabilities of instance segmentation and text prompts for generating masks for specific objects in images. Here, we modified it so it can perform multiple segmentations on remote sensing imagery.\n",
        "\n",
        "LangSAM is built upon the Meta model and the `segment-anything` repository. Furthermore, it uses the GroundingDINO detection model, providing a comprehensive and effective tool for object detection and image segmentation.\n",
        "\n",
        "The LangSAM package is user-friendly and designed to streamline and simplify the process of object detection and image segmentation.\n",
        "\n",
        "For more comprehensive information, further details, and examples on how to use LangSAM, you can visit Luca Medeiros's GitHub page at https://github.com/luca-medeiros/lang-segment-anything."
      ],
      "metadata": {
        "id": "WQ52uDfW6_vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "For those not running this on Google Colab, please ensure you have correctly installed GroundingDINO. You'll need Microsoft C++ Build Tools and Git. Follow the steps below:\n",
        "\n",
        "1. **Microsoft C++ Build Tools**: Install this software, which provides the tools necessary for compiling and linking C++ code, by visiting [this link](https://visualstudio.microsoft.com/visual-cpp-build-tools/).\n",
        "\n",
        "2. **Git**: Git is a distributed version control system that's used to track changes in source code during software development. Install it from the official Git website: [https://git-scm.com/](https://git-scm.com/).\n",
        "\n",
        "    - During the installation process, ensure you select the option to add Git to your system's PATH environment variable. This allows you to run Git commands from the command prompt or terminal on your system.\n",
        "\n",
        "After successfully installing these tools, you may need to restart your computer to ensure all changes take effect. This will facilitate the successful installation and functioning of GroundingDINO on your local environment."
      ],
      "metadata": {
        "id": "d1bkQYKn_bCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5St9Bb91_CM"
      },
      "outputs": [],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install rasterio geopandas fiona shapely torch torchvision segment-anything huggingface_hub \\\n",
        "    -U git+https://github.com/IDEA-Research/GroundingDINO.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import rasterio\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import groundingdino.datasets.transforms as T\n",
        "from PIL import Image\n",
        "from rasterio.plot import show\n",
        "from matplotlib.patches import Rectangle\n",
        "from groundingdino.models import build_model\n",
        "from groundingdino.util import box_ops\n",
        "from groundingdino.util.inference import predict\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.utils import clean_state_dict\n",
        "from huggingface_hub import hf_hub_download\n",
        "from segment_anything import sam_model_registry\n",
        "from segment_anything import SamPredictor\n",
        "from shapely.geometry import shape\n",
        "from rasterio.features import shapes"
      ],
      "metadata": {
        "id": "QkNnSGJr5YLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "SAM_MODELS = {\n",
        "    \"vit_h\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n",
        "    \"vit_l\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n",
        "    \"vit_b\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
        "}\n",
        "\n",
        "# Default cache path for model checkpoints\n",
        "CACHE_PATH = os.environ.get(\"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\"))"
      ],
      "metadata": {
        "id": "oqzuHPmW5sfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def load_model_hf(repo_id: str, filename: str, ckpt_config_filename: str, device: str = 'cpu') -> torch.nn.Module:\n",
        "    \"\"\"\n",
        "    Loads a model from HuggingFace Model Hub.\n",
        "\n",
        "    Parameters:\n",
        "    repo_id (str): Repository ID on HuggingFace Model Hub.\n",
        "    filename (str): Name of the model file in the repository.\n",
        "    ckpt_config_filename (str): Name of the config file for the model in the repository.\n",
        "    device (str): Device to load the model onto. Default is 'cpu'.\n",
        "\n",
        "    Returns:\n",
        "    torch.nn.Module: The loaded model.\n",
        "    \"\"\"\n",
        "    # Ensure the repo ID and filenames are valid\n",
        "    assert isinstance(repo_id, str) and repo_id, \"Invalid repository ID\"\n",
        "    assert isinstance(filename, str) and filename, \"Invalid model filename\"\n",
        "    assert isinstance(ckpt_config_filename, str) and ckpt_config_filename, \"Invalid config filename\"\n",
        "    \n",
        "    # Download the config file and build the model from it\n",
        "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
        "    args = SLConfig.fromfile(cache_config_file)\n",
        "    model = build_model(args)\n",
        "    model.to(device)\n",
        "    \n",
        "    # Download the model checkpoint and load it into the model\n",
        "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    checkpoint = torch.load(cache_file, map_location=device)\n",
        "    model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def transform_image(image: Image) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Transforms an image using standard transformations for image-based models.\n",
        "\n",
        "    Parameters:\n",
        "    image (Image): The PIL Image to be transformed.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The transformed image as a tensor.\n",
        "    \"\"\"\n",
        "    transform = T.Compose([\n",
        "        T.RandomResize([800], max_size=1333),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image_transformed, _ = transform(image, None)\n",
        "    return image_transformed"
      ],
      "metadata": {
        "id": "bAFYyflY35zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class definition for LangSAM\n",
        "class LangSAM:\n",
        "    \"\"\"\n",
        "    A Language-based Segment-Anything Model (LangSAM) class which combines GroundingDINO and SAM.\n",
        "    \"\"\"\n",
        "    def __init__(self, sam_type: str = \"vit_h\"):\n",
        "        \"\"\"\n",
        "        Initialize the LangSAM instance.\n",
        "\n",
        "        Parameters:\n",
        "        sam_type (str): Type of SAM model to use. Default is \"vit_h\".\n",
        "        \"\"\"\n",
        "        if sam_type not in SAM_MODELS:\n",
        "            raise ValueError(f\"Invalid SAM model type. Available options are {list(SAM_MODELS.keys())}.\")\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.build_groundingdino()\n",
        "        self.build_sam(sam_type)\n",
        "\n",
        "    def build_sam(self, sam_type: str):\n",
        "        \"\"\"\n",
        "        Build the SAM model.\n",
        "\n",
        "        Parameters:\n",
        "        sam_type (str): Type of SAM model to use.\n",
        "        \"\"\"\n",
        "        checkpoint_url = SAM_MODELS[sam_type]\n",
        "        sam = sam_model_registry[sam_type]()\n",
        "        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)\n",
        "        sam.load_state_dict(state_dict, strict=True)\n",
        "        sam.to(device=self.device)\n",
        "        self.sam = SamPredictor(sam)\n",
        "\n",
        "    def build_groundingdino(self):\n",
        "        \"\"\"\n",
        "        Build the GroundingDINO model.\n",
        "        \"\"\"\n",
        "        ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
        "        ckpt_filename = \"groundingdino_swinb_cogcoor.pth\"\n",
        "        ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
        "        self.groundingdino = load_model_hf(ckpt_repo_id, ckpt_filename, ckpt_config_filename, self.device)\n",
        "\n",
        "    def predict_dino(self, image_pil, text_prompt, box_threshold, text_threshold):\n",
        "        \"\"\"\n",
        "        Run the GroundingDINO model prediction.\n",
        "\n",
        "        Parameters:\n",
        "        image_pil (Image): Input PIL Image.\n",
        "        text_prompt (str): Text prompt for the model.\n",
        "        box_threshold (float): Box threshold for the prediction.\n",
        "        text_threshold (float): Text threshold for the prediction.\n",
        "\n",
        "        Returns:\n",
        "        Tuple containing boxes, logits, and phrases.\n",
        "        \"\"\"\n",
        "        image_trans = transform_image(image_pil)\n",
        "        boxes, logits, phrases = predict(model=self.groundingdino,\n",
        "                                         image=image_trans,\n",
        "                                         caption=text_prompt,\n",
        "                                         box_threshold=box_threshold,\n",
        "                                         text_threshold=text_threshold,\n",
        "                                         device=self.device)\n",
        "        W, H = image_pil.size\n",
        "        boxes = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H]).to(boxes.device)  # Ensure tensor is on the same device\n",
        "        return boxes, logits, phrases\n",
        "\n",
        "    def predict_sam(self, image_pil: Image, boxes: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Run the SAM model prediction.\n",
        "\n",
        "        Parameters:\n",
        "        image_pil (Image): Input PIL Image.\n",
        "        boxes (torch.Tensor): Tensor of bounding boxes.\n",
        "\n",
        "        Returns:\n",
        "        Masks tensor.\n",
        "        \"\"\"\n",
        "        image_array = np.array(image_pil)\n",
        "        self.sam.set_image(image_array)\n",
        "        transformed_boxes = self.sam.transform.apply_boxes_torch(boxes, image_array.shape[:2])\n",
        "        masks, _, _ = self.sam.predict_torch(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            boxes=transformed_boxes.to(self.sam.device),\n",
        "            multimask_output=False,\n",
        "        )\n",
        "        return masks.cpu()\n",
        "\n",
        "    def predict(self, image_pil: Image, text_prompt: str, box_threshold: float, text_threshold: float):\n",
        "        \"\"\"\n",
        "        Run both GroundingDINO and SAM model prediction.\n",
        "\n",
        "        Parameters:\n",
        "        image_pil (Image): Input PIL Image.\n",
        "        text_prompt (str): Text prompt for the model.\n",
        "        box_threshold (float): Box threshold for the prediction.\n",
        "        text_threshold (float): Text threshold for the prediction.\n",
        "\n",
        "        Returns:\n",
        "        Tuple containing masks, boxes, phrases, and logits.\n",
        "        \"\"\"\n",
        "        boxes, logits, phrases = self.predict_dino(image_pil, text_prompt, box_threshold, text_threshold)\n",
        "        masks = torch.tensor([])\n",
        "        if len(boxes) > 0:\n",
        "            masks = self.predict_sam(image_pil, boxes)\n",
        "            masks = masks.squeeze(1)\n",
        "        return masks, boxes, phrases, logits"
      ],
      "metadata": {
        "id": "j8OZos0s-THi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the georeferenced image\n",
        "with rasterio.open(\"Image.tif\") as src:\n",
        "    image_np = src.read().transpose((1, 2, 0))  # Convert rasterio image to numpy array\n",
        "    transform = src.transform  # Save georeferencing information\n",
        "    crs = src.crs  # Save the Coordinate Reference System"
      ],
      "metadata": {
        "id": "dCE6_bJ93_AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the text-prompt\n",
        "text_prompt = \"tree\""
      ],
      "metadata": {
        "id": "CyprzJLC3-2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = LangSAM()"
      ],
      "metadata": {
        "id": "BOkiwzL_-aFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects (*refer to line 4 in the next cell*). These threshold values range between 0 to 1 and are set while calling the predict method of the LangSAM class.\n",
        "\n",
        "`box_threshold`: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.\n",
        "\n",
        "`text_threshold`: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.\n",
        "\n",
        "Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall."
      ],
      "metadata": {
        "id": "3Kk44CyF8Pw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Segment the image\n",
        "image_pil = Image.fromarray(image_np[:, :, :3])  # Convert numpy array to PIL image, excluding the alpha channel\n",
        "\n",
        "masks, boxes, phrases, logits = model.predict(image_pil, text_prompt, box_threshold=0.3, text_threshold=0.3)  # Customize the box and text threshold values to your data\n",
        "\n",
        "if boxes.nelement() == 0:  # No \"object\" instances found\n",
        "    print('No objects found in the image.')\n",
        "else:\n",
        "    # Create an empty image to store the mask overlays\n",
        "    mask_overlay = np.zeros_like(image_np[..., 0], dtype=np.uint8)  # Adjusted for single channel\n",
        "\n",
        "    for i, (box, mask) in enumerate(zip(boxes, masks)):\n",
        "        # Convert tensor to numpy array if necessary and ensure it contains integers\n",
        "        if isinstance(mask, torch.Tensor):\n",
        "          mask = mask.cpu().numpy().astype(np.uint8)  # If mask is on GPU, use .cpu() before .numpy()\n",
        "        mask_overlay += ((mask > 0) * (i + 1)).astype(np.uint8)  # Assign a unique value for each mask\n",
        "\n",
        "    # Normalize mask_overlay to be in [0, 255]\n",
        "    mask_overlay = (mask_overlay > 0) * 255  # Binary mask in [0, 255]\n",
        "\n",
        "    # Display the original image with all mask overlays and bounding boxes\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image_pil)\n",
        "\n",
        "    for box in boxes:\n",
        "        # Draw bounding box\n",
        "        box = box.cpu().numpy()  # Convert the tensor to a numpy array\n",
        "        rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
        "        plt.gca().add_patch(rect)\n",
        "\n",
        "    plt.imshow(mask_overlay, cmap='viridis', alpha=0.4)  # Overlay the mask with some transparency\n",
        "    plt.title(f\"Segmented\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VychZxQk47yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the image as a GeoTIFF\n",
        "mask_overlay = ((mask_overlay > 0) * 255).astype(rasterio.uint8)  # Binary mask in [0, 255]\n",
        "\n",
        "# Save the mask_overlay as a new georeferenced raster\n",
        "with rasterio.open(\n",
        "    'mask.tif',\n",
        "    'w',\n",
        "    driver='GTiff',\n",
        "    height=mask_overlay.shape[0],\n",
        "    width=mask_overlay.shape[1],\n",
        "    count=1,\n",
        "    dtype=mask_overlay.dtype,\n",
        "    crs=crs,\n",
        "    transform=transform,\n",
        ") as dst:\n",
        "    dst.write(mask_overlay, 1)"
      ],
      "metadata": {
        "id": "3_gAdDMy4_FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the individual segmentations into a multi-part ShapeFile\n",
        "mask = mask_overlay.astype('int16')  # Convert the mask to integer type\n",
        "results = (\n",
        "    {'properties': {'raster_val': v}, 'geometry': s}\n",
        "    for i, (s, v) in enumerate(shapes(mask, transform=transform))\n",
        "    if v == 255  # Add condition to only keep 'trees'\n",
        ")\n",
        "\n",
        "geoms = list(results)\n",
        "\n",
        "gdf = gpd.GeoDataFrame.from_features(geoms)\n",
        "gdf.crs = crs  # Assign the Coordinate Reference System of the original image\n",
        "\n",
        "# Save to file\n",
        "gdf.to_file(\"mask.shp\")"
      ],
      "metadata": {
        "id": "qDZouxnA5BQd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}