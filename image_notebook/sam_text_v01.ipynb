{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNB6BNha3QscBqOds+MBvBJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasOsco/AI-RemoteSensing/blob/main/sam_text_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hME5Lj3nOs5e"
      },
      "outputs": [],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install rasterio\n",
        "!pip install torch torchvision\n",
        "!pip install segment-anything\n",
        "!pip install huggingface_hub\n",
        "!pip install -U git+https://github.com/IDEA-Research/GroundingDINO.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "import groundingdino.datasets.transforms as T\n",
        "from PIL import Image\n",
        "from rasterio.plot import show\n",
        "from matplotlib.patches import Rectangle\n",
        "from groundingdino.models import build_model\n",
        "from groundingdino.util import box_ops\n",
        "from groundingdino.util.inference import predict\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.utils import clean_state_dict\n",
        "from huggingface_hub import hf_hub_download\n",
        "from segment_anything import sam_model_registry\n",
        "from segment_anything import SamPredictor"
      ],
      "metadata": {
        "id": "1HMqbK9yOtd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "SAM_MODELS = {\n",
        "    \"vit_h\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n",
        "    \"vit_l\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n",
        "    \"vit_b\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
        "}\n",
        "\n",
        "CACHE_PATH = os.environ.get(\"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\"))"
      ],
      "metadata": {
        "id": "HKg6btGeOuRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
        "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
        "    args = SLConfig.fromfile(cache_config_file)\n",
        "    model = build_model(args)\n",
        "    model.to(device)\n",
        "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    checkpoint = torch.load(cache_file, map_location='cpu')\n",
        "    model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def transform_image(image) -> torch.Tensor:\n",
        "    transform = T.Compose([\n",
        "        T.RandomResize([800], max_size=1333),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image_transformed, _ = transform(image, None)\n",
        "    return image_transformed"
      ],
      "metadata": {
        "id": "yypxC6MkOu9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class definition for LangSAM\n",
        "class LangSAM():\n",
        "    def __init__(self, sam_type=\"vit_h\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.build_groundingdino()\n",
        "        self.build_sam(sam_type)\n",
        "\n",
        "    def build_sam(self, sam_type):\n",
        "        checkpoint_url = SAM_MODELS[sam_type]\n",
        "        sam = sam_model_registry[sam_type]()\n",
        "        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)\n",
        "        sam.load_state_dict(state_dict, strict=True)\n",
        "        sam.to(device=self.device)\n",
        "        self.sam = SamPredictor(sam)\n",
        "\n",
        "    def build_groundingdino(self):\n",
        "        ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
        "        ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
        "        ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
        "        self.groundingdino = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, self.device)\n",
        "\n",
        "    def predict_dino(self, image_pil, text_prompt, box_threshold, text_threshold):\n",
        "        image_trans = transform_image(image_pil)\n",
        "        boxes, logits, phrases = predict(model=self.groundingdino,\n",
        "                                         image=image_trans,\n",
        "                                         caption=text_prompt,\n",
        "                                         box_threshold=box_threshold,\n",
        "                                         text_threshold=text_threshold,\n",
        "                                         device=self.device)\n",
        "        W, H = image_pil.size\n",
        "        boxes = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
        "\n",
        "        return boxes, logits, phrases\n",
        "\n",
        "    def predict_sam(self, image_pil, boxes):\n",
        "        image_array = np.asarray(image_pil)\n",
        "        self.sam.set_image(image_array)\n",
        "        transformed_boxes = self.sam.transform.apply_boxes_torch(boxes, image_array.shape[:2])\n",
        "        masks, _, _ = self.sam.predict_torch(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            boxes=transformed_boxes.to(self.sam.device),\n",
        "            multimask_output=False,\n",
        "        )\n",
        "        return masks.cpu()\n",
        "\n",
        "    def predict(self, image_pil, text_prompt, box_threshold, text_threshold):\n",
        "        boxes, logits, phrases = self.predict_dino(image_pil, text_prompt, box_threshold, text_threshold)\n",
        "        masks = torch.tensor([])\n",
        "        if len(boxes) > 0:\n",
        "            masks = self.predict_sam(image_pil, boxes)\n",
        "            masks = masks.squeeze(1)\n",
        "        return masks, boxes, phrases, logits"
      ],
      "metadata": {
        "id": "5yvjLRw-OwjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the georeferenced image\n",
        "with rasterio.open(\"Image.tif\") as src:\n",
        "    image_np = src.read().transpose((1, 2, 0))  # Convert rasterio image to numpy array\n",
        "    transform = src.transform  # Save georeferencing information\n",
        "    crs = src.crs  # Save the Coordinate Reference System\n",
        "\n",
        "# Provide the text-prompt\n",
        "text_prompt = \"tree\""
      ],
      "metadata": {
        "id": "eIfR8aFxjylZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = LangSAM()"
      ],
      "metadata": {
        "id": "mokIQCrkO0JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_pil = Image.fromarray(image_np[:, :, :3]) # Convert numpy array to PIL image, excluding the alpha channel\n",
        "image_np_copy = image_np.copy()  # Create a copy for modifications\n",
        "\n",
        "masks, boxes, phrases, logits = model.predict(image_pil, text_prompt, box_threshold=0.3, text_threshold=0.3) # Customize the box and text threshold values to your data\n",
        "\n",
        "if boxes.nelement() == 0:  # No \"object\" instances found\n",
        "    print('No objects found in the image.')\n",
        "else:\n",
        "    # Create an empty image to store the mask overlays\n",
        "    mask_overlay = np.zeros_like(image_np[..., 0], dtype=np.int64)  # Adjusted for single channel\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes[i].cpu().numpy()  # Convert the tensor to a numpy array\n",
        "        mask = masks[i].cpu().numpy()  # Convert the tensor to a numpy array\n",
        "\n",
        "        # Add the mask to the mask_overlay image\n",
        "        mask_overlay += ((mask > 0) * (i + 1))  # Assign a unique value for each mask\n",
        "\n",
        "    # Normalize mask_overlay to be in [0, 255]\n",
        "    mask_overlay = (mask_overlay > 0) * 255  # Binary mask in [0, 255]\n",
        "\n",
        "    # Display the original image with all mask overlays and bounding boxes\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image_pil)\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes[i].cpu().numpy()  # Convert the tensor to a numpy array\n",
        "        # Draw bounding box\n",
        "        rect = Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
        "        plt.gca().add_patch(rect)\n",
        "\n",
        "    plt.imshow(mask_overlay, cmap='viridis', alpha=0.5)  # Overlay the mask with some transparency\n",
        "    plt.title(f\"Segmented\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VwH9MY45SMri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the image as a GeoTIFF\n",
        "mask_overlay = ((mask_overlay > 0) * 255).astype(rasterio.uint8)  # Binary mask in [0, 255]\n",
        "\n",
        "# Save the mask_overlay as a new georeferenced raster\n",
        "with rasterio.open(\n",
        "    'mask.tif',\n",
        "    'w',\n",
        "    driver='GTiff',\n",
        "    height=mask_overlay.shape[0],\n",
        "    width=mask_overlay.shape[1],\n",
        "    count=1,\n",
        "    dtype=mask_overlay.dtype,\n",
        "    crs=crs,\n",
        "    transform=transform,\n",
        ") as dst:\n",
        "    dst.write(mask_overlay, 1)"
      ],
      "metadata": {
        "id": "aQKVp69bkGHQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}